{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "05-transfer-learning-keras-kaggle.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQD18qO1JVhy",
        "outputId": "93ba1c98-7dc8-4836-8259-e5a6b2493421"
      },
      "source": [
        "#https://www.pyimagesearch.com/2019/05/20/transfer-learning-with-keras-and-deep-learning/\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "elif cpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
        "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical CPU, 1 Logical CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pob_7VXNJVh7",
        "outputId": "36a41aca-c1bb-4ab0-96ca-d6b66361a662"
      },
      "source": [
        "# +Add data before loading\n",
        "\n",
        "!wget -O gdrivedl 'https://f.mjh.nz/gdrivedl'\n",
        "!ls -l\n",
        "!pwd\n",
        "\n",
        "\"\"\"\n",
        "!bash /kaggle/working/gdrivedl https://drive.google.com/file/d/1UdpT3LhiomMs6PQVinCkRwSUC0hD3D19/view?usp=sharing\n",
        "!ls -l /kaggle/working\n",
        "inputdir = \"/kaggle/working\"\n",
        "\"\"\"\n",
        "\n",
        "!bash /content/gdrivedl https://drive.google.com/file/d/1UdpT3LhiomMs6PQVinCkRwSUC0hD3D19/view?usp=sharing\n",
        "!ls -l /content\n",
        "inputdir = \"/content\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-12 22:50:56--  https://f.mjh.nz/gdrivedl\n",
            "Resolving f.mjh.nz (f.mjh.nz)... 172.67.162.157, 104.28.30.233, 104.28.31.233, ...\n",
            "Connecting to f.mjh.nz (f.mjh.nz)|172.67.162.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1362 (1.3K) [application/octet-stream]\n",
            "Saving to: ‘gdrivedl’\n",
            "\n",
            "gdrivedl            100%[===================>]   1.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-12 22:50:56 (17.3 MB/s) - ‘gdrivedl’ saved [1362/1362]\n",
            "\n",
            "total 8\n",
            "-rw-r--r-- 1 root root 1362 Jan 29  2019 gdrivedl\n",
            "drwxr-xr-x 1 root root 4096 Dec  2 22:04 sample_data\n",
            "/content\n",
            "File ID: 1UdpT3LhiomMs6PQVinCkRwSUC0hD3D19\n",
            "Downloading: https://docs.google.com/uc?export=download&id=1UdpT3LhiomMs6PQVinCkRwSUC0hD3D19 > .87.file\n",
            "Downloading: https://docs.google.com/uc?export=download&id=1UdpT3LhiomMs6PQVinCkRwSUC0hD3D19&confirm=DrcI > .87.file\n",
            "Moving: .87.file > Food-5K.zip\n",
            "Saved: Food-5K.zip\n",
            "DONE!\n",
            "total 436476\n",
            "-rw-r--r-- 1 root root 446938948 Dec 12 22:51 Food-5K.zip\n",
            "-rw-r--r-- 1 root root      1362 Jan 29  2019 gdrivedl\n",
            "drwxr-xr-x 1 root root      4096 Dec  2 22:04 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUzQmSeEJVh9"
      },
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "\n",
        "# initialize the path to the *original* input directory of images\n",
        "ORIG_INPUT_DATASET = os.path.join(inputdir, \"Food-5K.zip\")\n",
        "\n",
        "# initialize the base path to the *new* directory that will contain\n",
        "# our images after computing the training and testing split\n",
        "import tempfile\n",
        "TEMPDIR = tempfile.gettempdir()\n",
        "BASE_PATH = os.path.join(TEMPDIR, \"dataset.zip\")\n",
        "\n",
        "# define the names of the training, testing, and validation\n",
        "# directories\n",
        "TRAIN = \"training\"\n",
        "TEST = \"evaluation\"\n",
        "VAL = \"validation\"\n",
        "\n",
        "# initialize the list of class label names\n",
        "CLASSES = [\"non_food\", \"food\"]\n",
        "\n",
        "# set the batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# initialize the label encoder file path and the output directory to\n",
        "# where the extracted features (in CSV file format) will be stored\n",
        "LE_PATH = os.path.sep.join([TEMPDIR, \"le.cpickle\"])\n",
        "#BASE_CSV_PATH = \"output\"\n",
        "BASE_CSV_PATH = TEMPDIR\n",
        "\n",
        "# set the path to the serialized model after training\n",
        "MODEL_PATH = os.path.sep.join([TEMPDIR, \"model.cpickle\"])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK_0cRPCJVh-",
        "outputId": "23a1aa1a-2e36-4ffd-c9cf-5a582f190a71"
      },
      "source": [
        "!pip install imutils"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW3LIO6TJVh-"
      },
      "source": [
        "# import the necessary packages\n",
        "from imutils import paths\n",
        "import shutil\n",
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.applications import VGG16\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLvMBN7YJVh_",
        "outputId": "f6545667-cd25-452a-bda0-ac4048bc3ed5"
      },
      "source": [
        "import zipfile\n",
        "import cv2\n",
        "zf = zipfile.ZipFile(ORIG_INPUT_DATASET)\n",
        "imagelist = zf.namelist()\n",
        "\n",
        "print(\"[INFO] processing split images...\")\n",
        "\n",
        "ozf = zipfile.ZipFile(BASE_PATH, 'w')\n",
        "\n",
        "# loop over the data splits\n",
        "for split in (TRAIN, TEST, VAL):\n",
        "\t# grab all image paths in the current split\n",
        "\t#print(\"[INFO] processing '{} split'...\".format(split))\n",
        "\t#p = os.path.sep.join([ORIG_INPUT_DATASET, split])\n",
        "\t#imagePaths = list(paths.list_images(p))\n",
        "\timagePaths = [ix for ix in imagelist if ix.startswith(split)]\n",
        "\n",
        "\t# loop over the image paths\n",
        "\tfor imagePath in imagePaths:\n",
        "\t\t# extract class label from the filename\n",
        "\t\t#filename = imagePath.split(os.path.sep)[-1]\n",
        "\t\tfilename = imagePath.split('/')[-1]\n",
        "\t\tlabel = CLASSES[int(filename.split(\"_\")[0])]\n",
        "\n",
        "\t\t# construct the path to the output directory\n",
        "\t\t#dirPath = os.path.sep.join([BASE_PATH, split, label])\n",
        "\t\tdirPath = '/'.join([split, label])\n",
        "\n",
        "\t\t# if the output directory does not exist, create it\n",
        "\t\t#if not os.path.exists(dirPath):\n",
        "\t\t#\tos.makedirs(dirPath)\n",
        "\n",
        "\t\t# construct the path to the output image file and copy it\n",
        "\t\t#p = os.path.sep.join([dirPath, filename])\n",
        "\t\t#shutil.copy2(imagePath, p)\n",
        "\t\t#image = cv2.imdecode(np.frombuffer(zf.read(imagePath), np.uint8), 1)\n",
        "\t\t#ozf.writestr(dirPath + '/' + filename, image, compress_type=zipfile.ZIP_STORED)\n",
        "\t\tozf.writestr(dirPath + '/' + filename, zf.read(imagePath), compress_type=zipfile.ZIP_STORED)\n",
        "zf.close()\n",
        "ozf.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing split images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EpMmu_KJVh_",
        "outputId": "5023ebbd-c46b-4a0a-9344-40a270fd97fe"
      },
      "source": [
        "# load the VGG16 network and initialize the label encoder\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0C8GbbjJVh_",
        "outputId": "365265be-2740-4b38-f0c2-90569478fc95"
      },
      "source": [
        "import cv2\n",
        "zf = zipfile.ZipFile(BASE_PATH)\n",
        "imagelist = zf.namelist()\n",
        "\n",
        "print(\"[INFO] processing split images...\")\n",
        "\n",
        "le = None\n",
        "\n",
        "# loop over the data splits\n",
        "for split in (TRAIN, TEST, VAL):\n",
        "\t# grab all image paths in the current split\n",
        "\t#print(\"[INFO] processing '{} split'...\".format(split))\n",
        "\t#p = os.path.sep.join([BASE_PATH, split])\n",
        "\t#imagePaths = list(paths.list_images(p))\n",
        "\timagePaths = [ix for ix in imagelist if ix.startswith(split)]\n",
        "\n",
        "\t# randomly shuffle the image paths and then extract the class\n",
        "\t# labels from the file paths\n",
        "\trandom.shuffle(imagePaths)\n",
        "\t#labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
        "\tlabels = [p.split('/')[1] for p in imagePaths]\n",
        "\tprint(imagePaths[:5], labels[:5])\n",
        "\n",
        "\t# if the label encoder is None, create it\n",
        "\tif le is None:\n",
        "\t\tle = LabelEncoder()\n",
        "\t\tle.fit(labels)\n",
        "\n",
        "\t# open the output CSV file for writing\n",
        "\tcsvPath = os.path.sep.join([BASE_CSV_PATH,\"{}.csv\".format(split)])\n",
        "\tcsv = open(csvPath, \"w\")\n",
        "\n",
        "\t# loop over the images in batches\n",
        "\tfor (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n",
        "\t\t# extract the batch of images and labels, then initialize the\n",
        "\t\t# list of actual images that will be passed through the network\n",
        "\t\t# for feature extraction\n",
        "\t\tprint(\"[INFO] processing batch {}/{}\".format(b + 1,int(np.ceil(len(imagePaths) / float(BATCH_SIZE)))))\n",
        "\t\tbatchPaths = imagePaths[i:i + BATCH_SIZE]\n",
        "\t\t#print(list(le.classes_), imagePaths[:5], labels[:5])\n",
        "\t\tbatchLabels = le.transform(labels[i:i + BATCH_SIZE])\n",
        "\t\tbatchImages = []\n",
        "\n",
        "\t\t# loop over the images and labels in the current batch\n",
        "\t\tfor imagePath in batchPaths:\n",
        "\t\t\t# load the input image using the Keras helper utility\n",
        "\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
        "\t\t\t#image = load_img(imagePath, target_size=(224, 224))\n",
        "\t\t\t#image = img_to_array(image)\n",
        "\t\t\timage = cv2.imdecode(np.frombuffer(zf.read(imagePath), np.uint8), 1)\n",
        "\t\t\timage = cv2.resize(image, (224, 224))\n",
        "\n",
        "\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
        "\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
        "\t\t\t# ImageNet dataset\n",
        "\t\t\timage = np.expand_dims(image, axis=0)\n",
        "\t\t\timage = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "\t\t\t# add the image to the batch\n",
        "\t\t\tbatchImages.append(image)\n",
        "\n",
        "\t\t# pass the images through the network and use the outputs as\n",
        "\t\t# our actual features, then reshape the features into a\n",
        "\t\t# flattened volume\n",
        "\t\tbatchImages = np.vstack(batchImages)\n",
        "\t\tfeatures = model.predict(batchImages, batch_size=BATCH_SIZE)\n",
        "\t\tprint(features.shape)\n",
        "\t\tfeatures = features.reshape((features.shape[0], 7 * 7 * 512))\n",
        "\n",
        "\t\t# loop over the class labels and extracted features\n",
        "\t\tfor (label, vec) in zip(batchLabels, features):\n",
        "\t\t\t# construct a row that exists of the class label and\n",
        "\t\t\t# extracted features\n",
        "\t\t\tvec = \",\".join([str(v) for v in vec])\n",
        "\t\t\tcsv.write(\"{},{}\\n\".format(label, vec))\n",
        "\n",
        "\t# close the CSV file\n",
        "\tcsv.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing split images...\n",
            "['training/non_food/0_564.jpg', 'training/non_food/0_1449.jpg', 'training/non_food/0_1233.jpg', 'training/non_food/0_1483.jpg', 'training/food/1_15.jpg'] ['non_food', 'non_food', 'non_food', 'non_food', 'food']\n",
            "[INFO] processing batch 1/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 2/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 3/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 4/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 5/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 6/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 7/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 8/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 9/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 10/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 11/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 12/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 13/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 14/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 15/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 16/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 17/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 18/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 19/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 20/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 21/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 22/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 23/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 24/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 25/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 26/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 27/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 28/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 29/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 30/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 31/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 32/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 33/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 34/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 35/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 36/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 37/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 38/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 39/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 40/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 41/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 42/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 43/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 44/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 45/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 46/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 47/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 48/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 49/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 50/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 51/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 52/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 53/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 54/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 55/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 56/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 57/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 58/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 59/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 60/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 61/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 62/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 63/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 64/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 65/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 66/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 67/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 68/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 69/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 70/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 71/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 72/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 73/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 74/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 75/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 76/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 77/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 78/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 79/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 80/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 81/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 82/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 83/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 84/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 85/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 86/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 87/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 88/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 89/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 90/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 91/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 92/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 93/94\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 94/94\n",
            "(24, 7, 7, 512)\n",
            "['evaluation/food/1_339.jpg', 'evaluation/non_food/0_297.jpg', 'evaluation/non_food/0_45.jpg', 'evaluation/food/1_204.jpg', 'evaluation/food/1_102.jpg'] ['food', 'non_food', 'non_food', 'food', 'food']\n",
            "[INFO] processing batch 1/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 2/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 3/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 4/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 5/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 6/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 7/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 8/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 9/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 10/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 11/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 12/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 13/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 14/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 15/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 16/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 17/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 18/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 19/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 20/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 21/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 22/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 23/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 24/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 25/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 26/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 27/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 28/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 29/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 30/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 31/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 32/32\n",
            "(8, 7, 7, 512)\n",
            "['validation/non_food/0_293.jpg', 'validation/food/1_33.jpg', 'validation/food/1_5.jpg', 'validation/food/1_284.jpg', 'validation/food/1_342.jpg'] ['non_food', 'food', 'food', 'food', 'food']\n",
            "[INFO] processing batch 1/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 2/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 3/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 4/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 5/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 6/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 7/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 8/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 9/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 10/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 11/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 12/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 13/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 14/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 15/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 16/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 17/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 18/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 19/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 20/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 21/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 22/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 23/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 24/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 25/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 26/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 27/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 28/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 29/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 30/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 31/32\n",
            "(32, 7, 7, 512)\n",
            "[INFO] processing batch 32/32\n",
            "(8, 7, 7, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNUiu07LJViA"
      },
      "source": [
        "# serialize the label encoder to disk\n",
        "f = open(LE_PATH, \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ycyVhgJViB"
      },
      "source": [
        "def load_data_split(splitPath):\n",
        "\t# initialize the data and labels\n",
        "\tdata = []\n",
        "\tlabels = []\n",
        "\n",
        "\t# loop over the rows in the data split file\n",
        "\tfor row in open(splitPath):\n",
        "\t\t# extract the class label and features from the row\n",
        "\t\trow = row.strip().split(\",\")\n",
        "\t\tlabel = row[0]\n",
        "\t\tfeatures = np.array(row[1:], dtype=\"float\")\n",
        "\n",
        "\t\t# update the data and label lists\n",
        "\t\tdata.append(features)\n",
        "\t\tlabels.append(label)\n",
        "\n",
        "\t# convert the data and labels to NumPy arrays\n",
        "\tdata = np.array(data)\n",
        "\tlabels = np.array(labels)\n",
        "\n",
        "\t# return a tuple of the data and labels\n",
        "\treturn (data, labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AyoC_-vJViB"
      },
      "source": [
        "# derive the paths to the training and testing CSV files\n",
        "trainingPath = os.path.sep.join([BASE_CSV_PATH,\"{}.csv\".format(TRAIN)])\n",
        "testingPath = os.path.sep.join([BASE_CSV_PATH,\"{}.csv\".format(TEST)])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXUJn8QfJViB",
        "outputId": "232357e0-8212-4be1-98d8-58e3843f4082"
      },
      "source": [
        "# load the data from disk\n",
        "print(\"[INFO] loading data...\")\n",
        "(trainX, trainY) = load_data_split(trainingPath)\n",
        "(testX, testY) = load_data_split(testingPath)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkbrOE-RJViC"
      },
      "source": [
        "# load the label encoder from disk\n",
        "le = pickle.loads(open(LE_PATH, \"rb\").read())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nie4Rv9qJViC",
        "outputId": "15e5d5f8-48f6-49e7-d960-a7680d6d59f4"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=1000)\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k20QcENGJViC",
        "outputId": "f4ccbe3c-3223-4d17-9cb2-ffec81e987a9"
      },
      "source": [
        "# evaluate the model\n",
        "print(\"[INFO] evaluating...\")\n",
        "preds = model.predict(testX)\n",
        "print(classification_report(testY, preds, target_names=le.classes_))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        food       0.98      0.97      0.97       500\n",
            "    non_food       0.97      0.98      0.98       500\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.98      0.97      0.97      1000\n",
            "weighted avg       0.98      0.97      0.97      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-FCLcg_JViD",
        "outputId": "516bb5de-32fa-47a3-d1e0-3c90f7200002"
      },
      "source": [
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "f = open(MODEL_PATH, \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u1ZABKUJViD"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}