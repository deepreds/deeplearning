{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "# pip install imutils\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "# <Option 1> You need to import GlobalAveragePooling2D if you want to use GAP model\n",
    "#from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StridedNet:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, reg, init=\"he_normal\"):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# our first CONV layer will learn a total of 16 filters, each\n",
    "\t\t# Of which are 7x7 -- we'll then apply 2x2 strides to reduce\n",
    "\t\t# the spatial dimensions of the volume\n",
    "\t\tmodel.add(Conv2D(16, (7, 7), strides=(2, 2), padding=\"valid\", kernel_initializer=init, kernel_regularizer=reg, input_shape=inputShape))\n",
    "\n",
    "\t\t# here we stack two CONV layers on top of each other where\n",
    "\t\t# each layerswill learn a total of 32 (3x3) filters\n",
    "\t\t#NK: <Option 2> You SHOULD design your own CNN models based on StrideNet (old) model\n",
    "\t\t#NK: Five components of Convolution layer: Conv2D, BatchNormalization, Maxpooling2D, Dropout, Flatten or GlobalAveragePooling2D\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=init, kernel_regularizer=reg))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t#NK: <Option 3> Conv2, same padding, stride (2,2) is essentially same as MaxPooling2D\n",
    "\t\t#NK: You can change following lines to MaxPooling2D\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=init, kernel_regularizer=reg))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\t\t#NK: stack two more CONV layers, keeping the size of each filter\n",
    "\t\t#NK: as 3x3 but increasing to 64 total learned filters\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=init, kernel_regularizer=reg))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t#NK: <Option 3> Conv2, same padding, stride (2,2) is essentially same as MaxPooling2D\n",
    "\t\t#NK: You can change following lines to MaxPooling2D\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=init, kernel_regularizer=reg))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# increase the number of filters again, this time to 128\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=init, kernel_regularizer=reg))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\t#NK: <Option 3> Conv2, same padding, stride (2,2) is essentially same as MaxPooling2D\n",
    "\t\t#NK: You can change following lines to MaxPooling2D\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=init, kernel_regularizer=reg))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t#NK: <Option 4> You can convert Flatten into GlobalAveragePooling2D, but you have to make sure that you have enough numbers of filters\n",
    "\t\t#NK: If the number of filter is small, performance of GlobalAveragePooling2D is not that good\n",
    "\t\t# fully-connected layer\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\t#NK: <Option 5> You can add FC (fully connected) layers, up to 2 times\n",
    "\t\t#NK: 3 or more FC layers are not recommended due to long calculation time and increase of parameters\n",
    "\t\tmodel.add(Dense(512, kernel_initializer=init))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\t#NK: <Option 6> You can change ratio of Dropout depending on how large is the image space you need in CNN model\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t#NK: <Caveat> DO NOT CHANGE THIS PART, FINAL LAYER SHOULD BE Dense(#class) with softmax\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the set of labels from the CALTECH-101 dataset we are\n",
    "# going to train our network on\n",
    "LABELS = set([\"Faces\", \"Leopards\", \"Motorbikes\", \"airplanes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagedir = 'CALTECH101_ObjectCategories'\n",
    "imagefilename = 'CALTECH101_ObjectCategories.zip'\n",
    "import zipfile\n",
    "print(\"[INFO] loading images...\")\n",
    "#imagePaths = list(paths.list_images(imagedir))\n",
    "zf = zipfile.ZipFile(imagefilename)\n",
    "imagelist = zf.namelist()\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the image paths\n",
    "#for imagePath in imagePaths:\n",
    "for imagePath in imagelist:\n",
    "\t# extract the class label from the filename\n",
    "\t#print(imagePath.split(os.path.sep), os.path.sep)\n",
    "\t#label = imagePath.split(os.path.sep)[-2]\n",
    "\tlabel = imagePath.split('/')[-2]\n",
    "\n",
    "\t# if the label of the current image is not part of of the labels\n",
    "\t# are interested in, then ignore the image\n",
    "\tif label not in LABELS:\n",
    "\t\tcontinue\n",
    "\n",
    "\t# load the image and resize it to be a fixed 96x96 pixels,\n",
    "\t# ignoring aspect ratio\n",
    "\timagedata = zf.read(imagePath)\n",
    "\t#image = cv2.imread(imagePath)\n",
    "\timage = cv2.imdecode(np.frombuffer(imagedata, np.uint8), 1)\n",
    "\timage = cv2.resize(image, (96, 96))\n",
    "\n",
    "\t# update the data and labels lists, respectively\n",
    "\tdata.append(image)\n",
    "\tlabels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into a NumPy array, then preprocess it by scaling\n",
    "# all pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "#NK: <Caveat> DO NOT CHANGE THIS PART, 25% OF DATA WILL BE USED AS TEST SET\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the training image generator for data augmentation\n",
    "#NK: You can do image augmentation further to increase your performance\n",
    "#NK: Check the option of ImageDataGenerator for further processing\n",
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NK: You can change # of epochs, optimizers, and regulators\n",
    "#NK: K2 regularizer Strong 0.01 ~ Week 0.0001\n",
    "maxepoch = 100\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=1e-4, decay=1e-4 / maxepoch)\n",
    "model = StridedNet.build(width=96, height=96, depth=3, classes=len(lb.classes_), reg=l2(0.0005))\n",
    "#NK: <Caveat> DO NOT CHANGE LOSS FUNCTION, SOFTWARE CLASSFIER ALWAYS USE CATEGORICAL_CROSSTNEROPY\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NK: You have to check dimension of each layers before doing model.fit\n",
    "#NK: Tensorflow will automatically calculate dimensional changes between layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NK: 25% of test dataset is feeded into CNN model by \"validation_data\"\n",
    "# train the network\n",
    "print(\"[INFO] training network for {} epochs...\".format(maxepoch))\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=10), validation_data=(testX, testY), steps_per_epoch=len(trainX) // 32, epochs=maxepoch, verbose=1)\n",
    "#batch_size: 32->10 for small GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NK: Your performance of CNN model will be evaluated by precision, recall and f1-score\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, maxepoch), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, maxepoch), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, maxepoch), H.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0, maxepoch), H.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
