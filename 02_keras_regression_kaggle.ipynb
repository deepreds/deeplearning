{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMgCZd7gEIp5",
    "outputId": "34bb4066-5a35-499c-986b-3d9cc662b99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical CPU, 1 Logical CPU\n"
     ]
    }
   ],
   "source": [
    "#https://www.pyimagesearch.com/2019/01/21/regression-with-keras/\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t6-U6nwhEIqC"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gd7h5TYFEIqD"
   },
   "outputs": [],
   "source": [
    "def load_house_attributes(inputPath):\n",
    "\t# initialize the list of column names in the CSV file and then\n",
    "\t# load it using Pandas\n",
    "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
    "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
    "\n",
    "\t# determine (1) the unique zip codes and (2) the number of data\n",
    "\t# points with each zip code\n",
    "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
    "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
    "\n",
    "\t# loop over each of the unique zip codes and their corresponding\n",
    "\t# count\n",
    "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
    "\t\t# the zip code counts for our housing dataset is *extremely*\n",
    "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
    "\t\t# so let's sanitize our data by removing any houses with less\n",
    "\t\t# than 25 houses per zip code\n",
    "\t\tif count < 25:\n",
    "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
    "\t\t\tdf.drop(idxs, inplace=True)\n",
    "\n",
    "\t# return the data frame\n",
    "\treturn df\n",
    "\n",
    "def process_house_attributes(df, train, test):\n",
    "\t# initialize the column names of the continuous data\n",
    "\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
    "\n",
    "\t# performin min-max scaling each continuous feature column to\n",
    "\t# the range [0, 1]\n",
    "\tcs = MinMaxScaler()\n",
    "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
    "\ttestContinuous = cs.transform(test[continuous])\n",
    "\n",
    "\t# one-hot encode the zip code categorical data (by definition of\n",
    "\t# one-hot encoing, all output features are now in the range [0, 1])\n",
    "\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
    "\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
    "\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
    "\n",
    "\t# construct our training and testing data points by concatenating\n",
    "\t# the categorical features with the continuous features\n",
    "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
    "\ttestX = np.hstack([testCategorical, testContinuous])\n",
    "\n",
    "\t# return the concatenated training and testing data\n",
    "\treturn (trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ue7XqPhiEIqD"
   },
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "\t# define our MLP network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "\tmodel.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "\t# return our model\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v6l9ZThiEIqD"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoCf4oGUEIqE",
    "outputId": "c1136129-c051-40fc-ecc3-46313613ec79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-12 22:28:30--  https://f.mjh.nz/gdrivedl\n",
      "Resolving f.mjh.nz (f.mjh.nz)... 104.28.31.233, 172.67.162.157, 104.28.30.233, ...\n",
      "Connecting to f.mjh.nz (f.mjh.nz)|104.28.31.233|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1362 (1.3K) [application/octet-stream]\n",
      "Saving to: ‘gdrivedl’\n",
      "\n",
      "gdrivedl            100%[===================>]   1.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-12 22:28:30 (14.2 MB/s) - ‘gdrivedl’ saved [1362/1362]\n",
      "\n",
      "total 12\n",
      "-rw-r--r-- 1 root root 1362 Jan 29  2019 gdrivedl\n",
      "-rw-r--r-- 1 root root 3261 Dec 12 22:28 google_drive.file\n",
      "drwxr-xr-x 1 root root 4096 Dec  2 22:04 sample_data\n",
      "/content\n",
      "File ID: 1LOiv7EyZspaKSWYiaKjUi81fr0Dqp0Si\n",
      "Downloading: https://docs.google.com/uc?export=download&id=1LOiv7EyZspaKSWYiaKjUi81fr0Dqp0Si > .111.file\n",
      "Downloading: https://docs.google.com/uc?export=download&id=1LOiv7EyZspaKSWYiaKjUi81fr0Dqp0Si&confirm=GbhJ > .111.file\n",
      "Moving: .111.file > Houses_Dataset.zip\n",
      "Saved: Houses_Dataset.zip\n",
      "DONE!\n",
      "total 181660\n",
      "-rw-r--r-- 1 root root      1362 Jan 29  2019 gdrivedl\n",
      "-rw-r--r-- 1 root root      3261 Dec 12 22:28 google_drive.file\n",
      "-rw-r--r-- 1 root root 186001776 Dec 12 22:28 Houses_Dataset.zip\n",
      "drwxr-xr-x 1 root root      4096 Dec  2 22:04 sample_data\n"
     ]
    }
   ],
   "source": [
    "# +Add data before loading\n",
    "\n",
    "!wget -O gdrivedl 'https://f.mjh.nz/gdrivedl'\n",
    "!ls -l\n",
    "!pwd\n",
    "\n",
    "\"\"\"\n",
    "!bash /kaggle/working/gdrivedl https://drive.google.com/file/d/1LOiv7EyZspaKSWYiaKjUi81fr0Dqp0Si/view?usp=sharing\n",
    "!ls -l /kaggle/working\n",
    "inputdir = \"/kaggle/working\"\n",
    "\"\"\"\n",
    "\n",
    "!bash /content/gdrivedl https://drive.google.com/file/d/1LOiv7EyZspaKSWYiaKjUi81fr0Dqp0Si/view?usp=sharing\n",
    "!ls -l /content\n",
    "inputdir = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsStxFUhEIqE",
    "outputId": "5cfa0912-c07e-41bd-f264-fd409968146b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading house attributes...\n"
     ]
    }
   ],
   "source": [
    "# construct the path to the input .txt file that contains information\n",
    "# on each house in the dataset and then load the dataset\n",
    "print(\"[INFO] loading house attributes...\")\n",
    "#inputPath = os.path.sep.join(['Houses_Dataset', \"HousesInfo.txt\"])\n",
    "import zipfile\n",
    "zf = zipfile.ZipFile(os.path.join(inputdir, 'Houses_Dataset.zip'))\n",
    "inputfile = zf.open('HousesInfo.txt')\n",
    "#df = load_house_attributes(inputPath)\n",
    "df = load_house_attributes(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73z7dQS3EIqF",
    "outputId": "8e8faa77-e322-4415-e491-eab2ad9f6dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] constructing training/testing split...\n"
     ]
    }
   ],
   "source": [
    "# construct a training and testing split with 75% of the data used\n",
    "# for training and the remaining 25% for evaluation\n",
    "print(\"[INFO] constructing training/testing split...\")\n",
    "(train, test) = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QhMj2h28EIqF"
   },
   "outputs": [],
   "source": [
    "# find the largest house price in the training set and use it to\n",
    "# scale our house prices to the range [0, 1] (this will lead to\n",
    "# better training and convergence)\n",
    "maxPrice = train[\"price\"].max()\n",
    "trainY = train[\"price\"] / maxPrice\n",
    "testY = test[\"price\"] / maxPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkBwQcQKEIqF",
    "outputId": "d02fd72e-44d1-4f69-9ee6-53a7a5194fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing data...\n"
     ]
    }
   ],
   "source": [
    "# process the house attributes data by performing min-max scaling\n",
    "# on continuous features, one-hot encoding on categorical features,\n",
    "# and then finally concatenating them together\n",
    "print(\"[INFO] processing data...\")\n",
    "(trainX, testX) = process_house_attributes(df, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "G__wMDrzEIqG"
   },
   "outputs": [],
   "source": [
    "# create our MLP and then compile the model using mean absolute\n",
    "# percentage error as our loss, implying that we seek to minimize\n",
    "# the absolute percentage difference between our price *predictions*\n",
    "# and the *actual prices*\n",
    "model = create_mlp(trainX.shape[1], regress=True)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5bt1Z7uEIqH",
    "outputId": "bd9762a4-ec2f-47fc-9190-47a3e3f336d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 129\n",
      "Trainable params: 129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtQCnMM-EIqH",
    "outputId": "85a7d4ce-74d1-49c4-edd4-dbf3d6fa2406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 70.0133 - val_loss: 57.1280\n",
      "Epoch 2/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 50.3408 - val_loss: 42.8583\n",
      "Epoch 3/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 42.5020 - val_loss: 36.8944\n",
      "Epoch 4/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 39.9217 - val_loss: 36.1380\n",
      "Epoch 5/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 38.2650 - val_loss: 35.5050\n",
      "Epoch 6/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 36.7163 - val_loss: 36.9688\n",
      "Epoch 7/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 35.9258 - val_loss: 31.4209\n",
      "Epoch 8/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 32.7223 - val_loss: 27.6826\n",
      "Epoch 9/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 31.2799 - val_loss: 29.2924\n",
      "Epoch 10/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 30.4881 - val_loss: 24.5982\n",
      "Epoch 11/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 28.3038 - val_loss: 23.5201\n",
      "Epoch 12/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 28.4367 - val_loss: 23.0413\n",
      "Epoch 13/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 28.2287 - val_loss: 24.3129\n",
      "Epoch 14/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 28.1653 - val_loss: 23.8568\n",
      "Epoch 15/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 28.4160 - val_loss: 21.9276\n",
      "Epoch 16/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 27.7382 - val_loss: 22.1714\n",
      "Epoch 17/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 26.6256 - val_loss: 21.2094\n",
      "Epoch 18/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 26.8459 - val_loss: 20.8325\n",
      "Epoch 19/200\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 26.4012 - val_loss: 21.3578\n",
      "Epoch 20/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 27.2645 - val_loss: 21.2607\n",
      "Epoch 21/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.7014 - val_loss: 20.2089\n",
      "Epoch 22/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 26.1059 - val_loss: 20.5564\n",
      "Epoch 23/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.7012 - val_loss: 22.1958\n",
      "Epoch 24/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.9976 - val_loss: 20.2925\n",
      "Epoch 25/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.8933 - val_loss: 23.9903\n",
      "Epoch 26/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.5179 - val_loss: 21.7317\n",
      "Epoch 27/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.9705 - val_loss: 20.1377\n",
      "Epoch 28/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.9118 - val_loss: 21.2235\n",
      "Epoch 29/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.0386 - val_loss: 20.1034\n",
      "Epoch 30/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.1566 - val_loss: 21.4059\n",
      "Epoch 31/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.7396 - val_loss: 22.8232\n",
      "Epoch 32/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.0149 - val_loss: 20.5107\n",
      "Epoch 33/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.5271 - val_loss: 20.9695\n",
      "Epoch 34/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.2487 - val_loss: 19.8485\n",
      "Epoch 35/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.3908 - val_loss: 20.3305\n",
      "Epoch 36/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.0731 - val_loss: 20.5838\n",
      "Epoch 37/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.4545 - val_loss: 21.1965\n",
      "Epoch 38/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.0056 - val_loss: 22.0649\n",
      "Epoch 39/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.9876 - val_loss: 21.3115\n",
      "Epoch 40/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.9188 - val_loss: 21.1502\n",
      "Epoch 41/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.2096 - val_loss: 25.5733\n",
      "Epoch 42/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.2468 - val_loss: 19.9642\n",
      "Epoch 43/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.0483 - val_loss: 21.5078\n",
      "Epoch 44/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.0939 - val_loss: 20.9329\n",
      "Epoch 45/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.8245 - val_loss: 20.1120\n",
      "Epoch 46/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.7346 - val_loss: 22.9212\n",
      "Epoch 47/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 25.2615 - val_loss: 19.8919\n",
      "Epoch 48/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.6953 - val_loss: 20.1829\n",
      "Epoch 49/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.5067 - val_loss: 20.0503\n",
      "Epoch 50/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.7945 - val_loss: 20.3767\n",
      "Epoch 51/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.4377 - val_loss: 19.1420\n",
      "Epoch 52/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.1441 - val_loss: 20.0666\n",
      "Epoch 53/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.3182 - val_loss: 21.1414\n",
      "Epoch 54/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.2243 - val_loss: 19.5297\n",
      "Epoch 55/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.2579 - val_loss: 19.2086\n",
      "Epoch 56/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.3635 - val_loss: 19.9407\n",
      "Epoch 57/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.2971 - val_loss: 19.9485\n",
      "Epoch 58/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.5403 - val_loss: 19.5781\n",
      "Epoch 59/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.4823 - val_loss: 19.8889\n",
      "Epoch 60/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.0895 - val_loss: 20.8605\n",
      "Epoch 61/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.3061 - val_loss: 19.2794\n",
      "Epoch 62/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.6230 - val_loss: 20.3319\n",
      "Epoch 63/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.0888 - val_loss: 21.0562\n",
      "Epoch 64/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.7724 - val_loss: 20.1990\n",
      "Epoch 65/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.9213 - val_loss: 20.7459\n",
      "Epoch 66/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.1598 - val_loss: 20.6881\n",
      "Epoch 67/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.5073 - val_loss: 19.4938\n",
      "Epoch 68/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.1186 - val_loss: 19.8300\n",
      "Epoch 69/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.5352 - val_loss: 19.2061\n",
      "Epoch 70/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.1037 - val_loss: 22.5046\n",
      "Epoch 71/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.8306 - val_loss: 20.7370\n",
      "Epoch 72/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.2445 - val_loss: 20.5745\n",
      "Epoch 73/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.0401 - val_loss: 20.5495\n",
      "Epoch 74/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.3123 - val_loss: 20.6274\n",
      "Epoch 75/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.3168 - val_loss: 19.6408\n",
      "Epoch 76/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.4498 - val_loss: 20.2209\n",
      "Epoch 77/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 24.0804 - val_loss: 22.5832\n",
      "Epoch 78/200\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 21.9295 - val_loss: 19.2941\n",
      "Epoch 79/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.2094 - val_loss: 20.4472\n",
      "Epoch 80/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.8056 - val_loss: 21.9514\n",
      "Epoch 81/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.1749 - val_loss: 22.4529\n",
      "Epoch 82/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.0599 - val_loss: 20.5963\n",
      "Epoch 83/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.0738 - val_loss: 19.8281\n",
      "Epoch 84/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.6614 - val_loss: 21.1850\n",
      "Epoch 85/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.2881 - val_loss: 20.5333\n",
      "Epoch 86/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.2431 - val_loss: 21.0256\n",
      "Epoch 87/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.8928 - val_loss: 20.8259\n",
      "Epoch 88/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.2864 - val_loss: 20.3979\n",
      "Epoch 89/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.2501 - val_loss: 20.3844\n",
      "Epoch 90/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 23.0893 - val_loss: 19.4776\n",
      "Epoch 91/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.3989 - val_loss: 20.1444\n",
      "Epoch 92/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.1551 - val_loss: 20.3615\n",
      "Epoch 93/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.5907 - val_loss: 20.0566\n",
      "Epoch 94/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.6396 - val_loss: 20.2731\n",
      "Epoch 95/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.8059 - val_loss: 22.0486\n",
      "Epoch 96/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.8740 - val_loss: 22.2383\n",
      "Epoch 97/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.5212 - val_loss: 22.6078\n",
      "Epoch 98/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.4748 - val_loss: 19.6924\n",
      "Epoch 99/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.7726 - val_loss: 20.5065\n",
      "Epoch 100/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.2547 - val_loss: 22.7711\n",
      "Epoch 101/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.6254 - val_loss: 21.0757\n",
      "Epoch 102/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.9738 - val_loss: 21.7941\n",
      "Epoch 103/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.2734 - val_loss: 20.4519\n",
      "Epoch 104/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.3469 - val_loss: 21.9391\n",
      "Epoch 105/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.4674 - val_loss: 22.7059\n",
      "Epoch 106/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.9916 - val_loss: 21.5033\n",
      "Epoch 107/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.9396 - val_loss: 20.7385\n",
      "Epoch 108/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.9503 - val_loss: 22.9258\n",
      "Epoch 109/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.3194 - val_loss: 22.0343\n",
      "Epoch 110/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.3887 - val_loss: 21.0659\n",
      "Epoch 111/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.9169 - val_loss: 21.2756\n",
      "Epoch 112/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.7958 - val_loss: 21.0890\n",
      "Epoch 113/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.2860 - val_loss: 21.7847\n",
      "Epoch 114/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.9304 - val_loss: 21.0058\n",
      "Epoch 115/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8975 - val_loss: 20.0204\n",
      "Epoch 116/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.9144 - val_loss: 23.5945\n",
      "Epoch 117/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.1476 - val_loss: 23.7761\n",
      "Epoch 118/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8347 - val_loss: 20.7090\n",
      "Epoch 119/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8255 - val_loss: 23.2424\n",
      "Epoch 120/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.9365 - val_loss: 21.1731\n",
      "Epoch 121/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.1109 - val_loss: 21.5724\n",
      "Epoch 122/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.5537 - val_loss: 21.8371\n",
      "Epoch 123/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.2670 - val_loss: 20.5912\n",
      "Epoch 124/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.2265 - val_loss: 19.8303\n",
      "Epoch 125/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.9056 - val_loss: 23.0415\n",
      "Epoch 126/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0441 - val_loss: 21.3691\n",
      "Epoch 127/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.2271 - val_loss: 20.5501\n",
      "Epoch 128/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5781 - val_loss: 21.8953\n",
      "Epoch 129/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5623 - val_loss: 22.5780\n",
      "Epoch 130/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.4375 - val_loss: 20.4541\n",
      "Epoch 131/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.4184 - val_loss: 20.6449\n",
      "Epoch 132/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 22.4568 - val_loss: 22.5374\n",
      "Epoch 133/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.6814 - val_loss: 20.7586\n",
      "Epoch 134/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.7555 - val_loss: 20.3592\n",
      "Epoch 135/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8852 - val_loss: 20.9844\n",
      "Epoch 136/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.3354 - val_loss: 23.1245\n",
      "Epoch 137/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8996 - val_loss: 20.4793\n",
      "Epoch 138/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8094 - val_loss: 20.9352\n",
      "Epoch 139/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5638 - val_loss: 22.9684\n",
      "Epoch 140/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0213 - val_loss: 20.3640\n",
      "Epoch 141/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5614 - val_loss: 22.4797\n",
      "Epoch 142/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4595 - val_loss: 21.3993\n",
      "Epoch 143/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0322 - val_loss: 21.4473\n",
      "Epoch 144/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.6070 - val_loss: 22.3201\n",
      "Epoch 145/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.9546 - val_loss: 28.5806\n",
      "Epoch 146/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.1637 - val_loss: 21.2055\n",
      "Epoch 147/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.9246 - val_loss: 22.3859\n",
      "Epoch 148/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.6435 - val_loss: 22.1455\n",
      "Epoch 149/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8528 - val_loss: 20.7515\n",
      "Epoch 150/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0992 - val_loss: 21.2349\n",
      "Epoch 151/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.7108 - val_loss: 19.9805\n",
      "Epoch 152/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.2809 - val_loss: 21.0468\n",
      "Epoch 153/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.8152 - val_loss: 22.8577\n",
      "Epoch 154/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5322 - val_loss: 21.4418\n",
      "Epoch 155/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4140 - val_loss: 20.4805\n",
      "Epoch 156/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5809 - val_loss: 20.5145\n",
      "Epoch 157/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.6804 - val_loss: 21.0603\n",
      "Epoch 158/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4350 - val_loss: 21.7482\n",
      "Epoch 159/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.2165 - val_loss: 20.9625\n",
      "Epoch 160/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0265 - val_loss: 20.8377\n",
      "Epoch 161/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.3436 - val_loss: 19.9289\n",
      "Epoch 162/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0441 - val_loss: 22.6918\n",
      "Epoch 163/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.6733 - val_loss: 23.1537\n",
      "Epoch 164/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.1789 - val_loss: 23.7884\n",
      "Epoch 165/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.7877 - val_loss: 25.9229\n",
      "Epoch 166/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.1378 - val_loss: 20.6668\n",
      "Epoch 167/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5991 - val_loss: 20.8728\n",
      "Epoch 168/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4118 - val_loss: 23.7552\n",
      "Epoch 169/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.2571 - val_loss: 21.3996\n",
      "Epoch 170/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0589 - val_loss: 21.0138\n",
      "Epoch 171/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.6340 - val_loss: 20.8916\n",
      "Epoch 172/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.6842 - val_loss: 21.4937\n",
      "Epoch 173/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4879 - val_loss: 20.7514\n",
      "Epoch 174/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.0914 - val_loss: 20.2160\n",
      "Epoch 175/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.2410 - val_loss: 20.3761\n",
      "Epoch 176/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 19.9060 - val_loss: 20.6373\n",
      "Epoch 177/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5240 - val_loss: 20.9308\n",
      "Epoch 178/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.7758 - val_loss: 20.2745\n",
      "Epoch 179/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4279 - val_loss: 19.8882\n",
      "Epoch 180/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4683 - val_loss: 19.9645\n",
      "Epoch 181/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.0923 - val_loss: 21.3179\n",
      "Epoch 182/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5034 - val_loss: 19.8690\n",
      "Epoch 183/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4018 - val_loss: 21.0163\n",
      "Epoch 184/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.6547 - val_loss: 22.5796\n",
      "Epoch 185/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 21.0827 - val_loss: 20.5812\n",
      "Epoch 186/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.7680 - val_loss: 22.6367\n",
      "Epoch 187/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5889 - val_loss: 21.9108\n",
      "Epoch 188/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.2715 - val_loss: 21.2713\n",
      "Epoch 189/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.3112 - val_loss: 19.8555\n",
      "Epoch 190/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 19.8667 - val_loss: 21.1662\n",
      "Epoch 191/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 19.8046 - val_loss: 20.4633\n",
      "Epoch 192/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 19.7119 - val_loss: 21.2163\n",
      "Epoch 193/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4506 - val_loss: 19.8931\n",
      "Epoch 194/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.1434 - val_loss: 22.4903\n",
      "Epoch 195/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.0872 - val_loss: 20.9883\n",
      "Epoch 196/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.0367 - val_loss: 20.1265\n",
      "Epoch 197/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.4017 - val_loss: 21.8030\n",
      "Epoch 198/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 19.9180 - val_loss: 21.3264\n",
      "Epoch 199/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.5611 - val_loss: 20.7901\n",
      "Epoch 200/200\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 20.0342 - val_loss: 22.6582\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(trainX, trainY, validation_data=(testX, testY),epochs=200, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Xb1GdmLQEIqI",
    "outputId": "142fa81d-89e2-4727-a3c0-07ae206e2ad0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zm-16Dc6EIqK",
    "outputId": "abb5403e-4805-41d8-d7d7-8bf8ea78b546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] predicting house prices...\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting house prices...\")\n",
    "preds = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "K2qLloDmEIqL"
   },
   "outputs": [],
   "source": [
    "# compute the difference between the *predicted* house prices and the\n",
    "# *actual* house prices, then compute the percentage difference and\n",
    "# the absolute percentage difference\n",
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eUEGZzhbEIqL"
   },
   "outputs": [],
   "source": [
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0gcsIESEIqM",
    "outputId": "80fa08d4-0d70-4968-e0a0-fd574534e534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] mean: 22.66%, std: 17.96%\n"
     ]
    }
   ],
   "source": [
    "# finally, show some statistics on our model\n",
    "#locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
    "#print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
    "#\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
    "#\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
    "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmQpUpJUEIqM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-keras-regression-kaggle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
